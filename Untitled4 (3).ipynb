{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1: Import Libraries\n",
    "\n",
    "This cell imports the necessary Python libraries for the project. These libraries provide various functions for data manipulation, text processing, and machine learning.\n",
    "\n",
    "-   `numpy`: For numerical operations.\n",
    "\n",
    "-   `pandas`: For data manipulation using DataFrames.\n",
    "-   `nltk`: Natural Language Toolkit for text processing (tokenization, stopwords, lemmatization).\n",
    "-   `re`: Regular expression operations for cleaning text.\n",
    "-   `string`: For string-related operations.\n",
    "-   `stopwords`: From `nltk.corpus`, for removing common English words.\n",
    "-   `word_tokenize`: From `nltk.tokenize`, for splitting text into words.\n",
    "-   `WordNetLemmatizer`: From `nltk.stem`, for reducing words to their base form.\n",
    "-   `TfidfVectorizer`: From `sklearn.feature_extraction.text`, for converting text to numerical features using TF-IDF.\n",
    "-   `train_test_split`: From `sklearn.model_selection`, for splitting data into training and testing sets.\n",
    "-   `SVC`: From `sklearn.svm`, the Support Vector Classifier (RBF kernel).\n",
    "-   `accuracy_score, classification_report`: From `sklearn.metrics`, for evaluating model performance.\n",
    "-   `Download NLTK Resources`:\n",
    "    -   `punkt`: Tokenizer for splitting text into sentences and words.\n",
    "    \n",
    "    -   `punkt_tab`: The punkt tokenizer relies on this model to determine where sentences end.\n",
    "    -   `stopwords`: A list of common English words to be removed from the text.\n",
    "    -   `wordnet`: A lexical database that helps in lemmatization (reducing words to their base or dictionary form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4837,
     "status": "ok",
     "timestamp": 1743431435682,
     "user": {
      "displayName": "Yash Bhatter",
      "userId": "14481163759016320843"
     },
     "user_tz": -330
    },
    "id": "dh663LsiDS32",
    "outputId": "503f4536-088d-4d7e-94dd-b206460777b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Yashasvi\n",
      "[nltk_data]     Acharya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Yashasvi\n",
      "[nltk_data]     Acharya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yashasvi\n",
      "[nltk_data]     Acharya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Yashasvi\n",
      "[nltk_data]     Acharya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer  # Using lemmatizer instead of stemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  # For lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Initial Exploration\n",
    "\n",
    "This cell loads the training and test datasets using pandas. The data is assumed to be in a \":::\" separated format. It also displays the first few rows of each dataset and provides some basic information about the training dataset.\n",
    "\n",
    "-   Loads data from the specified paths, using \":::\" as the separator.\n",
    "\n",
    "-   Assigns column names to the DataFrame.\n",
    "-   Displays the first few rows of the data.\n",
    "-   Prints info about the data like the type and the non null count.\n",
    "-   Prints the number of missing values in each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1088,
     "status": "ok",
     "timestamp": 1743431441232,
     "user": {
      "displayName": "Yash Bhatter",
      "userId": "14481163759016320843"
     },
     "user_tz": -330
    },
    "id": "-qxLKUOrDbo3",
    "outputId": "0be0b60d-4485-4de0-9df3-2957b0eb189d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "   ID                               TITLE       GENRE  \\\n",
      "0   1       Oscar et la dame rose (2009)       drama    \n",
      "1   2                       Cupid (1997)    thriller    \n",
      "2   3   Young, Wild and Wonderful (1980)       adult    \n",
      "3   4              The Secret Sin (1915)       drama    \n",
      "4   5             The Unrecovered (2007)       drama    \n",
      "\n",
      "                                         DESCRIPTION  \n",
      "0   Listening in to a conversation between his do...  \n",
      "1   A brother and sister with a past incestuous r...  \n",
      "2   As the bus empties the students for their fie...  \n",
      "3   To help their unemployed father make ends mee...  \n",
      "4   The film's title refers not only to the un-re...  \n",
      "\n",
      "Test Data:\n",
      "   ID                          TITLE  \\\n",
      "0   1          Edgar's Lunch (1998)    \n",
      "1   2      La guerra de pap√° (1977)    \n",
      "2   3   Off the Beaten Track (2010)    \n",
      "3   4        Meu Amigo Hindu (2015)    \n",
      "4   5             Er nu zhai (1955)    \n",
      "\n",
      "                                         DESCRIPTION  \n",
      "0   L.R. Brane loves his life - his car, his apar...  \n",
      "1   Spain, March 1964: Quico is a very naughty ch...  \n",
      "2   One year in the life of Albin and his family ...  \n",
      "3   His father has died, he hasn't spoken with hi...  \n",
      "4   Before he was known internationally as a mart...  \n",
      "\n",
      "Training Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54214 entries, 0 to 54213\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ID           54214 non-null  int64 \n",
      " 1   TITLE        54214 non-null  object\n",
      " 2   GENRE        54214 non-null  object\n",
      " 3   DESCRIPTION  54214 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.7+ MB\n",
      "None\n",
      "\n",
      "Missing Values in Training Data:\n",
      "ID             0\n",
      "TITLE          0\n",
      "GENRE          0\n",
      "DESCRIPTION    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load training and test data with correct column names\n",
    "train_path = \"train_data.txt\"\n",
    "test_path = \"test_data.txt\"\n",
    "\n",
    "train_data = pd.read_csv(train_path, sep=\":::\", names=[\"ID\", \"TITLE\", \"GENRE\", \"DESCRIPTION\"], engine=\"python\")\n",
    "test_data = pd.read_csv(test_path, sep=\":::\", names=[\"ID\", \"TITLE\", \"DESCRIPTION\"], engine=\"python\")  # Assuming no GENRE in test data\n",
    "\n",
    "# Display first few rows\n",
    "print(\"Training Data:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_data.head())\n",
    "\n",
    "# Check for missing values and data info\n",
    "print(\"\\nTraining Data Info:\")\n",
    "print(train_data.info())\n",
    "print(\"\\nMissing Values in Training Data:\")\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Function\n",
    "\n",
    "This cell defines a function called `clean_text` that performs text cleaning and preprocessing. This function takes a text string as input and performs the following operations:\n",
    "\n",
    "-   Handle Non-String Input: Checks if the input is a string. If not, it returns an empty string.\n",
    "\n",
    "-   Lowercase Conversion: Converts the text to lowercase.\n",
    "-   Remove Punctuation and Numbers: Removes any character that is not a letter or whitespace.\n",
    "-   Tokenization: Splits the text into individual words (tokens).\n",
    "-   Lemmatization and Stopword Removal: Reduces each word to its base form (lemma) and removes common English stop words.\n",
    "-   Join Tokens: Joins the cleaned tokens back into a single string.\n",
    "\n",
    "This function helps standardize the text data and remove noise, making it more suitable for machine learning models. It is applied to the \"DESCRIPTION\" column of both the training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76142,
     "status": "ok",
     "timestamp": 1743431521938,
     "user": {
      "displayName": "Yash Bhatter",
      "userId": "14481163759016320843"
     },
     "user_tz": -330
    },
    "id": "uH4pgy1iDh6U",
    "outputId": "aeedc451-b3c7-4931-c50a-d5ffafb209f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Processed Training Description:\n",
      "listening conversation doctor parent yearold oscar learns nobody courage tell week live furious refuse speak anyone except straighttalking rose lady pink meet hospital stair christmas approach rose us fantastical experience professional wrestler imagination wit charm allow oscar live life love full company friend pop corn einstein bacon childhood sweetheart peggy blue\n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Handle non-string input (e.g., NaN)\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize and remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "train_data[\"TextCleaning\"] = train_data[\"DESCRIPTION\"].apply(clean_text)\n",
    "test_data[\"TextCleaning\"] = test_data[\"DESCRIPTION\"].apply(clean_text)\n",
    "\n",
    "# Display a sample\n",
    "print(\"Sample Processed Training Description:\")\n",
    "print(train_data[\"TextCleaning\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing The Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization\n",
    "\n",
    "This cell uses the TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer to convert the cleaned text data into numerical features that can be used by the machine learning model.\n",
    "\n",
    "-   TfidfVectorizer: Converts text documents into a matrix of TF-IDF features.\n",
    "\n",
    "-   max_features: Limits the number of features to the top 'n' most frequent terms.\n",
    "-   ngram_range=(1, 2): Consider both unigrams and bigrams.\n",
    "\n",
    "The code then fits the vectorizer on the training data and transforms both the training and test data into TF-IDF feature matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18331,
     "status": "ok",
     "timestamp": 1743431545622,
     "user": {
      "displayName": "Yash Bhatter",
      "userId": "14481163759016320843"
     },
     "user_tz": -330
    },
    "id": "breDin91DkY6",
    "outputId": "53526676-0acd-4a03-fd6d-010678122db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (43371, 10000)\n",
      "Validation Features Shape: (10843, 10000)\n",
      "Test Features Shape: (54200, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = train_data[\"TextCleaning\"]\n",
    "y = train_data[\"GENRE\"]\n",
    "X_train_split, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_split)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(test_data[\"TextCleaning\"])\n",
    "\n",
    "print(\"Training Features Shape:\", X_train.shape)\n",
    "print(\"Validation Features Shape:\", X_val.shape)\n",
    "print(\"Test Features Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Logistic Regression\n",
    "\n",
    "This cell performs hyperparameter tuning for Logistic Regression using Grid Search. It finds the best value for the regularization parameter C using cross-validation.\n",
    "\n",
    "-   `param_grid`: Defines the hyperparameters and their possible values to search over.\n",
    "\n",
    "    *   `C`: Regularization parameter.\n",
    "\n",
    "-   `GridSearchCV`: Performs an exhaustive search over the specified parameter grid.\n",
    "\n",
    "    *   `LogisticRegression()`: The Logistic Regression model.\n",
    "    \n",
    "    *   `param_grid`: The hyperparameter grid.\n",
    "    *   `cv=3`: 3-fold cross-validation.\n",
    "    *   `scoring='accuracy'`: Uses accuracy as the scoring metric.\n",
    "\n",
    "The code then fits the `GridSearchCV` object on the training data to find the best combination of hyperparameters. It prints the best parameters and the best cross-validation score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3QxKivMEUF2",
    "outputId": "e4a35dfe-9a16-45e6-9b3a-35793e75753b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Grid Search for Logistic Regression...\n",
      "Grid Search completed.\n",
      "Best Parameters: {'C': 10.0}\n",
      "Best Cross-Validation Accuracy: 0.5375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for C\n",
    "param_grid = {'C': [0.1, 1.0, 10.0]}\n",
    "\n",
    "# Initialize the base model\n",
    "base_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# Set up Grid Search with 3-fold cross-validation\n",
    "grid = GridSearchCV(estimator=base_model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Train with Grid Search\n",
    "print(\"Running Grid Search for Logistic Regression...\")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Grid Search completed.\")\n",
    "\n",
    "# Extract the best model\n",
    "model = grid.best_estimator_\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Metrics\n",
    "\n",
    "This cell evaluates the performance of the trained Logistic Regression model.\n",
    "\n",
    "-   Training accuracy: accuracy of the model on the training data\n",
    "\n",
    "-   Validation accuracy: accuracy of the model on the validation data\n",
    "-   Classification report: precision, recall and f1-score for each class on the validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1743412965997,
     "user": {
      "displayName": "Yash Bhatter",
      "userId": "14481163759016320843"
     },
     "user_tz": -330
    },
    "id": "f-8b9SFwdlwO",
    "outputId": "4768243e-068a-4d99-9356-8936708e7c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8076\n",
      "Validation Accuracy: 0.5395\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      action        0.34      0.44      0.38       263\n",
      "       adult        0.51      0.57      0.54       112\n",
      "   adventure        0.19      0.29      0.23       139\n",
      "   animation        0.24      0.24      0.24       104\n",
      "   biography        0.07      0.05      0.06        61\n",
      "      comedy        0.56      0.56      0.56      1443\n",
      "       crime        0.17      0.23      0.19       107\n",
      " documentary        0.78      0.68      0.73      2659\n",
      "       drama        0.68      0.53      0.59      2697\n",
      "      family        0.19      0.29      0.23       150\n",
      "     fantasy        0.18      0.12      0.14        74\n",
      "   game-show        0.71      0.75      0.73        40\n",
      "     history        0.05      0.04      0.05        45\n",
      "      horror        0.58      0.70      0.63       431\n",
      "       music        0.45      0.69      0.54       144\n",
      "     musical        0.17      0.20      0.19        50\n",
      "     mystery        0.10      0.11      0.10        56\n",
      "        news        0.11      0.12      0.11        34\n",
      "  reality-tv        0.36      0.52      0.42       192\n",
      "     romance        0.14      0.20      0.16       151\n",
      "      sci-fi        0.38      0.44      0.41       143\n",
      "       short        0.40      0.45      0.42      1045\n",
      "       sport        0.42      0.57      0.48        93\n",
      "   talk-show        0.34      0.37      0.36        81\n",
      "    thriller        0.25      0.36      0.29       309\n",
      "         war        0.12      0.20      0.15        20\n",
      "     western        0.81      0.87      0.84       200\n",
      "\n",
      "     accuracy                           0.54     10843\n",
      "    macro avg       0.34      0.39      0.36     10843\n",
      " weighted avg       0.57      0.54      0.55     10843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = model.score(X_train, y_train)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "y_pred_val = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, vectorizer, and label encoder saved.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit LabelEncoder on training labels and transform\n",
    "le.fit(train_data[\"GENRE\"])\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'movie_genre_model.pkl')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# Save the LabelEncoder\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "\n",
    "print(\"Model, vectorizer, and label encoder saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Save Predictions\n",
    "\n",
    "This cell uses the trained Logistic Regression model to generate predictions on the test data and saves the predictions to a CSV file.\n",
    "\n",
    "-   Test the model on the test data\n",
    "\n",
    "-   Create a submission file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1743412972068,
     "user": {
      "displayName": "Yash Bhatter",
      "userId": "14481163759016320843"
     },
     "user_tz": -330
    },
    "id": "bApvcfh1dnwC",
    "outputId": "cfcd7d86-4d38-4c2c-e0da-63d0cfd42eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'submission.csv'.\n",
      "Sample Predictions:\n",
      "   ID                          TITLE PREDICTED_GENRE\n",
      "0   1          Edgar's Lunch (1998)          comedy \n",
      "1   2      La guerra de pap√° (1977)           drama \n",
      "2   3   Off the Beaten Track (2010)     documentary \n",
      "3   4        Meu Amigo Hindu (2015)           drama \n",
      "4   5             Er nu zhai (1955)         romance \n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "test_data['PREDICTED_GENRE'] = test_predictions\n",
    "\n",
    "output = test_data[['ID', 'TITLE', 'PREDICTED_GENRE']]\n",
    "output.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to 'submission.csv'.\")\n",
    "print(\"Sample Predictions:\")\n",
    "print(output.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, vectorizer, and label encoder saved.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit LabelEncoder on training labels and transform\n",
    "le.fit(train_data[\"GENRE\"])\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'movie_genre_model.pkl')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# Save the LabelEncoder\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "\n",
    "print(\"Model, vectorizer, and label encoder saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM76m/ZDLh7Hq5L/DJ0c7fR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
